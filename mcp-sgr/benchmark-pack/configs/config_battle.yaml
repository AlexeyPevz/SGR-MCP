# SGR Battle: Top Models vs Budget Models

models:
  # TOP TIER - The Champions
  - id: "openai/gpt-4o"
    name: "GPT-4o"
    type: "top"
    cost_per_1k: 0.0025
    
  - id: "anthropic/claude-3.5-sonnet"
    name: "Claude-3.5-Sonnet"
    type: "top"
    cost_per_1k: 0.003
    
  # MID TIER - The Balanced
  - id: "openai/gpt-3.5-turbo"
    name: "GPT-3.5-Turbo"
    type: "mid"
    cost_per_1k: 0.0005
    
  - id: "anthropic/claude-3-haiku"
    name: "Claude-3-Haiku"
    type: "mid"
    cost_per_1k: 0.00025
    
  # BUDGET TIER - The Underdogs
  - id: "mistralai/mistral-7b-instruct:free"
    name: "Mistral-7B-Free"
    type: "budget"
    cost_per_1k: 0.0
    
  - id: "ministral/ministral-8b-2410"
    name: "Ministral-8B"
    type: "budget"
    cost_per_1k: 0.00002
    
  - id: "deepseek/deepseek-chat"
    name: "DeepSeek-Chat"
    type: "budget"
    cost_per_1k: 0.00014
    
  - id: "qwen/qwen-2.5-7b-instruct"
    name: "Qwen-2.5-7B"
    type: "budget"
    cost_per_1k: 0.00018

# SGR modes
sgr_modes:
  - name: "off"
    description: "Baseline without SGR"
    
  - name: "lite"
    description: "Lightweight structured guidance"
    schema_fields: ["task_understanding", "solution"]
    
  - name: "full"
    description: "Comprehensive structured analysis"
    schema_fields: ["requirements_analysis", "approach", "implementation", "validation"]

# Categories for testing
categories:
  - name: "codegen"
    type: "primary"
    
  - name: "rag"
    type: "secondary"
    
  - name: "summarization"
    type: "secondary"

# Test settings
test_settings:
  max_retries: 2
  timeout: 60
  temperature: 0.7

# Evaluation metrics
evaluation:
  metrics:
    - accuracy
    - completeness
    - coherence
  composite_scoring: true

# Reporting options
reporting:
  formats:
    - json
    - markdown
  include_artifacts: false
  visualization: true