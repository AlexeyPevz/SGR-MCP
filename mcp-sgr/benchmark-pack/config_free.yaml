# SGR Benchmark Configuration - FREE/BUDGET VERSION
# Optimized for minimal cost with free and very cheap models

models:
  # Free models (via OpenRouter)
  - id: "mistralai/mistral-7b-instruct:free"
    name: "Mistral-7B-Free"
    type: "small"
    cost_per_1k: 0.0
    
  - id: "google/gemma-7b-it:free"
    name: "Gemma-7B-Free"
    type: "small"
    cost_per_1k: 0.0
    
  - id: "meta-llama/llama-3-8b-instruct:free"
    name: "Llama-3-8B-Free"
    type: "small"
    cost_per_1k: 0.0
    
  # Ultra cheap models ($0.00018-0.0002 per 1k tokens)
  - id: "openai/gpt-3.5-turbo"
    name: "GPT-3.5-Turbo"
    type: "medium"
    cost_per_1k: 0.0005  # ~$0.50 per 1M tokens
    
  - id: "deepseek/deepseek-chat"
    name: "DeepSeek-Chat"
    type: "large"
    cost_per_1k: 0.00014  # Incredibly cheap!
    
  - id: "qwen/qwen-2.5-7b-instruct"
    name: "Qwen-2.5-7B"
    type: "small"
    cost_per_1k: 0.00018

sgr_modes:
  - name: "off"
    description: "No structured guidance"
    
  - name: "lite"
    description: "Minimal structure"
    schema_fields: ["task_understanding", "solution"]
    
  - name: "full"
    description: "Full structured reasoning"
    schema_fields: ["requirements_analysis", "approach", "implementation", "validation"]

# Reduced task set for budget testing
test_categories:
  code_generation:
    count: 3  # Instead of 20
    tasks: ["code_simple_001", "code_api_002", "code_test_003"]
    
  rag_qa:
    count: 3  # Instead of 20
    tasks: ["rag_simple_001", "rag_citation_002", "rag_conflict_003"]
    
  summarization:
    count: 2  # Instead of 10
    tasks: ["sum_report_001", "sum_compare_007"]
    
  planning_decision:
    count: 2  # Instead of 10
    tasks: ["plan_arch_001", "plan_product_007"]
    
  data_etl:
    count: 2  # Instead of 10
    tasks: ["data_clean_001", "data_transform_007"]
    
  agent_workflow:
    count: 2  # Instead of 10
    tasks: ["agent_code_001", "agent_rag_007"]

# Total: 14 tasks instead of 80

evaluation:
  runs_per_task: 1  # Reduced from 3
  temperature: 0.1
  max_tokens: 2000  # Reduced from 4000
  timeout: 60  # Reduced from 120
  
cost_estimates:
  # With 14 tasks × 6 models × 3 modes × 1 run = 252 API calls
  # Average ~2000 tokens per call = 504,000 tokens total
  
  free_models_cost: "$0.00"  # Mistral, Gemma, Llama - completely free
  cheap_models_cost: "$0.17"  # GPT-3.5, DeepSeek, Qwen
  total_estimated: "< $0.20"  # Less than a cup of coffee!
  
quick_test:
  # Even smaller subset for initial testing
  tasks: ["code_simple_001", "rag_simple_001"]
  models: ["mistralai/mistral-7b-instruct:free", "deepseek/deepseek-chat"]
  modes: ["off", "lite"]
  # Total: 2 tasks × 2 models × 2 modes = 8 API calls (~$0.002)

reporting:
  formats: ["markdown", "json"]
  include_cost_analysis: true
  highlight_free_models: true