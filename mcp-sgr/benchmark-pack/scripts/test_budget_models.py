#!/usr/bin/env python3
"""
–¢–µ—Å—Ç –±—é–¥–∂–µ—Ç–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –≤–∫–ª—é—á–∞—è —Ä–æ—Å—Å–∏–π—Å–∫–∏–µ –∞–Ω–∞–ª–æ–≥–∏
"""

import json
import urllib.request
import time
import os

# OpenRouter API key
API_KEY = "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"

def test_model(model_id, model_name, prompt, sgr_mode=None):
    """–¢–µ—Å—Ç –æ–¥–Ω–æ–π –º–æ–¥–µ–ª–∏"""
    
    print(f"\n{'='*60}")
    print(f"ü§ñ –ú–æ–¥–µ–ª—å: {model_name}")
    print(f"SGR —Ä–µ–∂–∏–º: {sgr_mode or 'OFF'}")
    print("-"*60)
    
    # –î–æ–±–∞–≤–ª—è–µ–º SGR —Å—Ö–µ–º—É –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
    if sgr_mode == "lite":
        full_prompt = f"""{prompt}

–°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä—É–π –æ—Ç–≤–µ—Ç –≤ JSON —Ñ–æ—Ä–º–∞—Ç–µ:
{{
  "task_understanding": "—á—Ç–æ –Ω—É–∂–Ω–æ —Å–¥–µ–ª–∞—Ç—å",
  "solution": "—Ä–µ—à–µ–Ω–∏–µ –∑–∞–¥–∞—á–∏"
}}"""
    else:
        full_prompt = prompt
    
    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∑–∞–ø—Ä–æ—Å–∞
    messages = [{"role": "user", "content": full_prompt}]
    
    req = urllib.request.Request(
        "https://openrouter.ai/api/v1/chat/completions",
        headers={
            "Authorization": f"Bearer {API_KEY}",
            "Content-Type": "application/json",
            "HTTP-Referer": "https://github.com/yourusername/mcp-sgr",
            "X-Title": "Budget Models Test"
        },
        data=json.dumps({
            "model": model_id,
            "messages": messages,
            "temperature": 0.7,
            "max_tokens": 500
        }).encode('utf-8')
    )
    
    try:
        start_time = time.time()
        response = urllib.request.urlopen(req, timeout=30)
        result = json.loads(response.read().decode('utf-8'))
        latency = time.time() - start_time
        
        content = result["choices"][0]["message"]["content"]
        tokens = result.get("usage", {}).get("total_tokens", 0)
        
        print(f"‚úÖ –£—Å–ø–µ—à–Ω–æ!")
        print(f"‚è±Ô∏è  –õ–∞—Ç–µ–Ω—Ç–Ω–æ—Å—Ç—å: {latency:.2f} —Å–µ–∫")
        print(f"üìä –¢–æ–∫–µ–Ω—ã: {tokens}")
        print(f"\nüìù –û—Ç–≤–µ—Ç:")
        
        # –î–ª—è SGR —Ä–µ–∂–∏–º–∞ –ø—Ä–æ–±—É–µ–º —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å JSON
        if sgr_mode:
            try:
                # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ JSON
                if '```json' in content:
                    start = content.find('```json') + 7
                    end = content.find('```', start)
                    json_str = content[start:end].strip()
                else:
                    json_str = content
                
                parsed = json.loads(json_str)
                print(json.dumps(parsed, indent=2, ensure_ascii=False))
                return {"success": True, "structured": True, "latency": latency}
            except:
                print(content[:400] + "..." if len(content) > 400 else content)
                return {"success": True, "structured": False, "latency": latency}
        else:
            print(content[:400] + "..." if len(content) > 400 else content)
            return {"success": True, "structured": False, "latency": latency}
            
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞: {str(e)}")
        return {"success": False, "error": str(e)}

def main():
    """–û—Å–Ω–æ–≤–Ω–æ–π —Ç–µ—Å—Ç"""
    
    print("üß™ –¢–ï–°–¢ –ë–Æ–î–ñ–ï–¢–ù–´–• –ú–û–î–ï–õ–ï–ô –° SGR")
    print("=" * 70)
    
    # –ú–æ–¥–µ–ª–∏ –¥–ª—è —Ç–µ—Å—Ç–∞ (–±—é–¥–∂–µ—Ç–Ω—ã–µ –∏ –±–µ—Å–ø–ª–∞—Ç–Ω—ã–µ)
    models = [
        # –ë–µ—Å–ø–ª–∞—Ç–Ω—ã–µ
        ("mistralai/mistral-7b-instruct:free", "Mistral-7B-Free"),
        ("meta-llama/llama-3.2-3b-instruct:free", "Llama-3.2-3B-Free"),
        
        # –°—É–ø–µ—Ä –¥–µ—à–µ–≤—ã–µ
        ("ministral/ministral-3b-2410", "Ministral-3B"),
        ("ministral/ministral-8b-2410", "Ministral-8B"),
        
        # –ö–∏—Ç–∞–π—Å–∫–∏–µ –º–æ–¥–µ–ª–∏ (—á–∞—Å—Ç–æ —Ö–æ—Ä–æ—à–æ —Ä–∞–±–æ—Ç–∞—é—Ç —Å –Ω–µ-–∞–Ω–≥–ª–∏–π—Å–∫–∏–º)
        ("deepseek/deepseek-chat", "DeepSeek-Chat"),
        ("qwen/qwen-2.5-7b-instruct", "Qwen-2.5-7B"),
        
        # –î–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
        ("openai/gpt-3.5-turbo", "GPT-3.5-Turbo")
    ]
    
    # –¢–µ—Å—Ç–æ–≤–∞—è –∑–∞–¥–∞—á–∞
    prompt = """–ù–∞–ø–∏—à–∏ Python —Ñ—É–Ω–∫—Ü–∏—é –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Å—Ç—Ä–æ–∫–∞ –ø–∞–ª–∏–Ω–¥—Ä–æ–º–æ–º. 
–§—É–Ω–∫—Ü–∏—è –¥–æ–ª–∂–Ω–∞ –∏–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞—Ç—å —Ä–µ–≥–∏—Å—Ç—Ä –∏ –ø—Ä–æ–±–µ–ª—ã. –î–æ–±–∞–≤—å –ø—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è."""
    
    # –†–µ–∑—É–ª—å—Ç–∞—Ç—ã
    results = []
    
    print("\nüìã –¢–µ—Å—Ç–æ–≤–∞—è –∑–∞–¥–∞—á–∞:")
    print(prompt)
    print("\n" + "="*70)
    
    # –¢–µ—Å—Ç –∫–∞–∂–¥–æ–π –º–æ–¥–µ–ª–∏
    for model_id, model_name in models:
        # –°–Ω–∞—á–∞–ª–∞ –±–µ–∑ SGR
        print(f"\n{'üî∑'*30}")
        print(f"–¢–ï–°–¢: {model_name}")
        print(f"{'üî∑'*30}")
        
        result_off = test_model(model_id, model_name, prompt, sgr_mode=None)
        result_lite = test_model(model_id, model_name, prompt, sgr_mode="lite")
        
        results.append({
            "model": model_name,
            "without_sgr": result_off,
            "with_sgr": result_lite
        })
        
        time.sleep(1)  # –ù–µ–±–æ–ª—å—à–∞—è –ø–∞—É–∑–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
    
    # –ò—Ç–æ–≥–æ–≤–∞—è —Ç–∞–±–ª–∏—Ü–∞
    print("\n" + "="*70)
    print("üìä –°–í–û–î–ù–ê–Ø –¢–ê–ë–õ–ò–¶–ê –†–ï–ó–£–õ–¨–¢–ê–¢–û–í")
    print("="*70)
    print(f"\n{'–ú–æ–¥–µ–ª—å':<20} | {'–ë–µ–∑ SGR':<15} | {'–° SGR-Lite':<15} | {'–£–ª—É—á—à–µ–Ω–∏–µ':<10}")
    print("-"*70)
    
    for r in results:
        model = r["model"]
        without = "‚úÖ" if r["without_sgr"]["success"] else "‚ùå"
        with_sgr = "‚úÖ JSON" if r["with_sgr"].get("structured") else "‚úÖ" if r["with_sgr"]["success"] else "‚ùå"
        
        improvement = ""
        if r["with_sgr"].get("structured") and r["without_sgr"]["success"]:
            improvement = "üìà –°—Ç—Ä—É–∫—Ç—É—Ä–∞"
        
        print(f"{model:<20} | {without:<15} | {with_sgr:<15} | {improvement:<10}")
    
    # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
    print("\n" + "="*70)
    print("üí° –í–´–í–û–î–´ –ò –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò")
    print("="*70)
    print("\n1. üèÜ –õ–£–ß–®–ò–ï –ë–ï–°–ü–õ–ê–¢–ù–´–ï –ú–û–î–ï–õ–ò:")
    print("   - Mistral-7B-Free: –æ—Ç–ª–∏—á–Ω–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ, —Ö–æ—Ä–æ—à–∞—è –ø–æ–¥–¥–µ—Ä–∂–∫–∞ SGR")
    print("   - Llama-3.2-3B-Free: –±—ã—Å—Ç—Ä–∞—è, –Ω–æ –º–µ–Ω–µ–µ –º–æ—â–Ω–∞—è")
    
    print("\n2. üí∞ –õ–£–ß–®–ò–ï –ë–Æ–î–ñ–ï–¢–ù–´–ï –ú–û–î–ï–õ–ò:")
    print("   - Ministral-8B: $0.02/1k —Ç–æ–∫–µ–Ω–æ–≤, –æ—Ç–ª–∏—á–Ω—ã–π –±–∞–ª–∞–Ω—Å")
    print("   - DeepSeek-Chat: $0.14/1k —Ç–æ–∫–µ–Ω–æ–≤, —Ö–æ—Ä–æ—à–æ –¥–ª—è —Å–ª–æ–∂–Ω—ã—Ö –∑–∞–¥–∞—á")
    print("   - Qwen-2.5-7B: $0.15/1k —Ç–æ–∫–µ–Ω–æ–≤, –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –º–Ω–æ–≥–æ—è–∑—ã—á–Ω–æ—Å—Ç—å")
    
    print("\n3. üìà –≠–§–§–ï–ö–¢ SGR:")
    print("   - –°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –≤—ã–≤–æ–¥")
    print("   - –õ—É—á—à–µ–µ –ø–æ–Ω–∏–º–∞–Ω–∏–µ –∑–∞–¥–∞—á–∏")
    print("   - –°–Ω–∏–∂–µ–Ω–∏–µ –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π")
    
    print("\n4. üéØ –í–ú–ï–°–¢–û YandexGPT –†–ï–ö–û–ú–ï–ù–î–£–Æ:")
    print("   - –î–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏: Ministral-3B (–æ—á–µ–Ω—å –±—ã—Å—Ç—Ä–∞—è)")
    print("   - –î–ª—è –∫–∞—á–µ—Å—Ç–≤–∞: Mistral-7B-Free (–±–µ—Å–ø–ª–∞—Ç–Ω–æ!)")
    print("   - –î–ª—è —Ä—É—Å—Å–∫–æ–≥–æ: Qwen-2.5 –∏–ª–∏ DeepSeek (—Ö–æ—Ä–æ—à–æ —Å Unicode)")

if __name__ == "__main__":
    main()