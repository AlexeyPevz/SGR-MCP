#!/usr/bin/env python3
"""
SGR vs Baseline Benchmark - comparing models with and without SGR v4
"""

import json
import os
import time
import urllib.request
from typing import Dict, List, Tuple, Optional
from datetime import datetime
import sys
sys.path.append('/workspace/mcp-sgr/src/tools')
from apply_sgr_v4 import TASK_SCHEMAS

OPENROUTER_API_KEY = os.getenv("OPENROUTER_API_KEY")

# –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫ –º–æ–¥–µ–ª–µ–π –≤–∫–ª—é—á–∞—è –±—é–¥–∂–µ—Ç–Ω—ã–µ
BENCHMARK_MODELS = [
    # Premium models
    {"name": "Claude-3.5-Sonnet", "id": "anthropic/claude-3.5-sonnet", "size": "large", "cost": 0.003},
    {"name": "GPT-4-Turbo", "id": "openai/gpt-4-turbo", "size": "large", "cost": 0.01},
    
    # Large open models
    {"name": "Qwen-2.5-72B", "id": "qwen/qwen-2.5-72b-instruct", "size": "72B", "cost": 0.0003},
    {"name": "Llama-3.1-70B", "id": "meta-llama/llama-3.1-70b-instruct", "size": "70B", "cost": 0.0003},
    {"name": "DeepSeek-V2.5", "id": "deepseek/deepseek-chat", "size": "236B", "cost": 0.00014},
    
    # Medium models
    {"name": "Mixtral-8x7B", "id": "mistralai/mixtral-8x7b-instruct", "size": "47B", "cost": 0.00024},
    {"name": "Qwen-2.5-32B", "id": "qwen/qwen-2.5-32b-instruct", "size": "32B", "cost": 0.0002},
    {"name": "Gemini-Flash-1.5", "id": "google/gemini-flash-1.5", "size": "unknown", "cost": 0.0003},
    
    # Small models
    {"name": "Qwen-2.5-7B", "id": "qwen/qwen-2.5-7b-instruct", "size": "7B", "cost": 0.00007},
    {"name": "Mistral-7B", "id": "mistralai/mistral-7b-instruct", "size": "7B", "cost": 0.00007},
    {"name": "Gemma-2-9B", "id": "google/gemma-2-9b-it", "size": "9B", "cost": 0.0001},
    {"name": "Llama-3.2-3B", "id": "meta-llama/llama-3.2-3b-instruct", "size": "3B", "cost": 0.00006},
]

# Test tasks covering different types
BENCHMARK_TASKS = {
    "code_review": {
        "name": "Security Code Review",
        "task": """Review this authentication code:

```python
def login(request):
    username = request.POST.get('username')
    password = request.POST.get('password')
    
    # Check user
    user = db.query(f"SELECT * FROM users WHERE username='{username}' AND password=MD5('{password}')")
    
    if user:
        # Create token
        token = base64.b64encode(f"{username}:{time.time()}".encode()).decode()
        
        # Log successful login
        os.system(f"echo 'Login: {username}' >> /var/log/auth.log")
        
        return {"token": token, "user": user}
    
    return {"error": "Invalid credentials"}
```

Analyze for security vulnerabilities and best practices.""",
        "expected_issues": ["sql_injection", "weak_hashing", "command_injection", "predictable_token"]
    },
    
    "system_design": {
        "name": "Microservices Design",
        "task": """Design a URL shortener service with these requirements:

1. Handle 100M URLs/day
2. Custom aliases support
3. Analytics (click tracking)
4. < 50ms latency
5. 99.99% availability
6. Expire URLs after 2 years

Design the architecture, data model, and API.""",
        "expected_components": ["load_balancer", "cache", "database", "analytics", "api_design"]
    },
    
    "debugging": {
        "name": "Memory Leak Debug",
        "task": """Our Node.js API server has a memory leak:

- Memory usage grows from 200MB to 2GB over 24 hours
- Happens only in production, not in dev
- Started after adding new image processing feature
- The feature resizes user uploaded images
- We use Sharp library for image processing
- Memory doesn't decrease even during low traffic

What's the likely cause and fix?""",
        "expected_insights": ["buffer_not_released", "stream_not_closed", "event_listeners", "sharp_cache"]
    }
}


def call_model_baseline(model: Dict, task: str) -> Dict:
    """Call model without SGR (baseline)."""
    
    start_time = time.time()
    
    messages = [
        {"role": "system", "content": "You are an expert assistant. Provide thorough analysis."},
        {"role": "user", "content": task}
    ]
    
    data = {
        "model": model["id"],
        "messages": messages,
        "temperature": 0.1,
        "max_tokens": 2000
    }
    
    request = urllib.request.Request(
        "https://openrouter.ai/api/v1/chat/completions",
        data=json.dumps(data).encode('utf-8'),
        headers={
            "Authorization": f"Bearer {OPENROUTER_API_KEY}",
            "Content-Type": "application/json"
        }
    )
    
    try:
        with urllib.request.urlopen(request, timeout=60) as response:
            result = json.loads(response.read().decode('utf-8'))
            content = result["choices"][0]["message"]["content"]
            tokens = result.get("usage", {}).get("total_tokens", 0)
            
            return {
                "success": True,
                "content": content,
                "duration": time.time() - start_time,
                "tokens": tokens,
                "cost": tokens * model["cost"] / 1000,
                "error": None
            }
            
    except Exception as e:
        return {
            "success": False,
            "content": None,
            "duration": time.time() - start_time,
            "tokens": 0,
            "cost": 0,
            "error": str(e)[:100]
        }


def call_model_with_sgr(model: Dict, task: str, task_type: str, schema: Dict) -> Dict:
    """Call model with SGR v4."""
    
    start_time = time.time()
    
    system_prompt = """You are an expert providing structured analysis.

Your response MUST be valid JSON matching the provided schema. The schema guides your reasoning - each field represents a critical aspect to analyze.

Be thorough and specific. Respond ONLY with valid JSON."""
    
    user_prompt = f"""Task: {task}

Analyze this as a {task_type.replace('_', ' ')} task.

Structure your response as JSON matching this schema:
{json.dumps(schema, indent=2)}"""
    
    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": user_prompt}
    ]
    
    data = {
        "model": model["id"],
        "messages": messages,
        "temperature": 0.1,
        "max_tokens": 2000
    }
    
    request = urllib.request.Request(
        "https://openrouter.ai/api/v1/chat/completions",
        data=json.dumps(data).encode('utf-8'),
        headers={
            "Authorization": f"Bearer {OPENROUTER_API_KEY}",
            "Content-Type": "application/json"
        }
    )
    
    try:
        with urllib.request.urlopen(request, timeout=60) as response:
            result = json.loads(response.read().decode('utf-8'))
            content = result["choices"][0]["message"]["content"]
            tokens = result.get("usage", {}).get("total_tokens", 0)
            
            # Try to parse JSON
            parsed = None
            if content.strip().startswith("{"):
                try:
                    parsed = json.loads(content)
                except:
                    pass
            elif "```json" in content:
                try:
                    json_str = content.split("```json")[1].split("```")[0]
                    parsed = json.loads(json_str)
                except:
                    pass
                    
            return {
                "success": parsed is not None,
                "content": content,
                "parsed": parsed,
                "duration": time.time() - start_time,
                "tokens": tokens,
                "cost": tokens * model["cost"] / 1000,
                "error": None if parsed else "Failed to parse JSON"
            }
            
    except Exception as e:
        return {
            "success": False,
            "content": None,
            "parsed": None,
            "duration": time.time() - start_time,
            "tokens": 0,
            "cost": 0,
            "error": str(e)[:100]
        }


def evaluate_response(response: str, task_type: str, expected: List[str]) -> Dict[str, float]:
    """Evaluate response quality."""
    
    if not response:
        return {"score": 0, "found": []}
    
    response_lower = response.lower()
    found = []
    
    # Check for expected elements
    for item in expected:
        # Use various forms of the expected item
        variations = [
            item,
            item.replace("_", " "),
            item.replace("_", "-"),
            item.split("_")[0] if "_" in item else item
        ]
        
        if any(var in response_lower for var in variations):
            found.append(item)
    
    # Calculate score
    score = (len(found) / len(expected)) * 10 if expected else 5
    
    # Bonus for structure and depth
    structure_bonus = 0
    if any(marker in response_lower for marker in ["1.", "2.", "3.", "first", "second", "third"]):
        structure_bonus += 1
    if len(response) > 1000:
        structure_bonus += 1
    if any(word in response_lower for word in ["however", "additionally", "furthermore", "consider"]):
        structure_bonus += 1
        
    final_score = min(10, score + structure_bonus)
    
    return {
        "score": final_score,
        "found": found,
        "coverage": len(found) / len(expected) if expected else 0
    }


def main():
    """Run comprehensive SGR vs Baseline benchmark."""
    
    print("\nüî¨ SGR vs Baseline Comprehensive Benchmark")
    print("=" * 80)
    print(f"Testing {len(BENCHMARK_MODELS)} models on {len(BENCHMARK_TASKS)} tasks")
    print("Comparing performance WITH and WITHOUT SGR")
    print("=" * 80)
    
    results = []
    
    # Test subset of models to save time/cost
    test_models = [
        BENCHMARK_MODELS[2],  # Qwen-2.5-72B
        BENCHMARK_MODELS[4],  # DeepSeek
        BENCHMARK_MODELS[5],  # Mixtral
        BENCHMARK_MODELS[8],  # Qwen-2.5-7B
        BENCHMARK_MODELS[10], # Gemma-2-9B
    ]
    
    for task_type, task_info in BENCHMARK_TASKS.items():
        print(f"\n\nüìã Task: {task_info['name']}")
        print("=" * 80)
        print(f"Preview: {task_info['task'][:150]}...")
        
        schema = TASK_SCHEMAS.get(task_type, TASK_SCHEMAS["general_reasoning"])
        
        for i, model in enumerate(test_models):
            print(f"\n{i+1}/{len(test_models)} Testing {model['name']} ({model['size']})")
            
            # Test WITHOUT SGR
            print("  üî¥ Baseline (no SGR):", end="", flush=True)
            baseline = call_model_baseline(model, task_info["task"])
            
            if baseline["success"]:
                baseline_eval = evaluate_response(
                    baseline["content"], 
                    task_type,
                    task_info.get("expected_issues", task_info.get("expected_components", task_info.get("expected_insights", [])))
                )
                print(f" ‚úì Score: {baseline_eval['score']:.1f}/10, Time: {baseline['duration']:.1f}s")
                print(f"     Found: {', '.join(baseline_eval['found'][:3])}")
            else:
                baseline_eval = {"score": 0, "found": []}
                print(f" ‚úó Failed: {baseline['error']}")
            
            time.sleep(1)
            
            # Test WITH SGR
            print("  üü¢ With SGR v4:", end="", flush=True)
            sgr = call_model_with_sgr(model, task_info["task"], task_type, schema)
            
            if sgr["success"]:
                # For SGR, check structured response
                sgr_eval = {"score": 8, "found": ["structured_output"]}  # Base score for successful JSON
                
                if task_type == "code_review" and sgr["parsed"]:
                    vulns = sgr["parsed"].get("security_analysis", {}).get("vulnerabilities", [])
                    sgr_eval["score"] = min(10, 5 + len(vulns))
                    sgr_eval["found"] = [v.get("type", "unknown") for v in vulns[:3]]
                
                print(f" ‚úì Score: {sgr_eval['score']:.1f}/10, Time: {sgr['duration']:.1f}s")
                print(f"     Structured: Yes, Fields: {len(sgr['parsed']) if sgr['parsed'] else 0}")
            else:
                sgr_eval = {"score": 0, "found": []}
                print(f" ‚úó Failed: {sgr['error']}")
            
            # Calculate improvement
            improvement = ((sgr_eval["score"] - baseline_eval["score"]) / max(baseline_eval["score"], 0.1)) * 100
            print(f"  üìä SGR Impact: {improvement:+.1f}% ({baseline_eval['score']:.1f} ‚Üí {sgr_eval['score']:.1f})")
            
            results.append({
                "model": model["name"],
                "size": model["size"],
                "task": task_type,
                "baseline_score": baseline_eval["score"],
                "sgr_score": sgr_eval["score"],
                "improvement": improvement,
                "baseline_cost": baseline["cost"],
                "sgr_cost": sgr["cost"],
                "baseline_success": baseline["success"],
                "sgr_success": sgr["success"]
            })
            
            time.sleep(1)
    
    # Summary
    print("\n\n" + "="*80)
    print("üìä OVERALL RESULTS")
    print("="*80)
    
    # Group by model
    model_stats = {}
    for r in results:
        model = r["model"]
        if model not in model_stats:
            model_stats[model] = {
                "improvements": [],
                "baseline_scores": [],
                "sgr_scores": [],
                "total_cost": 0
            }
        
        model_stats[model]["improvements"].append(r["improvement"])
        model_stats[model]["baseline_scores"].append(r["baseline_score"])
        model_stats[model]["sgr_scores"].append(r["sgr_score"])
        model_stats[model]["total_cost"] += r["baseline_cost"] + r["sgr_cost"]
    
    # Calculate averages and display
    print("\nüèÜ Model Performance Summary:")
    print("-" * 90)
    print(f"{'Model':<20} {'Baseline':<12} {'With SGR':<12} {'Improvement':<15} {'Total Cost'}")
    print("-" * 90)
    
    model_summaries = []
    for model, stats in model_stats.items():
        avg_baseline = sum(stats["baseline_scores"]) / len(stats["baseline_scores"])
        avg_sgr = sum(stats["sgr_scores"]) / len(stats["sgr_scores"])
        avg_improvement = sum(stats["improvements"]) / len(stats["improvements"])
        
        model_summaries.append({
            "model": model,
            "baseline": avg_baseline,
            "sgr": avg_sgr,
            "improvement": avg_improvement,
            "cost": stats["total_cost"]
        })
        
        print(f"{model:<20} {avg_baseline:<12.1f} {avg_sgr:<12.1f} {avg_improvement:<15.1f}% ${stats['total_cost']:.4f}")
    
    # Best performers
    print("\nü•á Top Performers:")
    print("-" * 50)
    
    # Best improvement
    best_improvement = max(model_summaries, key=lambda x: x["improvement"])
    print(f"1. Highest SGR Improvement: {best_improvement['model']} (+{best_improvement['improvement']:.1f}%)")
    
    # Best overall
    best_overall = max(model_summaries, key=lambda x: x["sgr"])
    print(f"2. Best Overall Score: {best_overall['model']} ({best_overall['sgr']:.1f}/10)")
    
    # Best value
    value_scores = []
    for m in model_summaries:
        if m["cost"] > 0:
            value = m["sgr"] / m["cost"]
            value_scores.append({"model": m["model"], "value": value, "score": m["sgr"], "cost": m["cost"]})
    
    if value_scores:
        best_value = max(value_scores, key=lambda x: x["value"])
        print(f"3. Best Value: {best_value['model']} ({best_value['value']:.0f} points/$)")
    
    # Task-specific insights
    print("\nüìà SGR Effectiveness by Task Type:")
    print("-" * 50)
    
    task_improvements = {}
    for r in results:
        task = r["task"]
        if task not in task_improvements:
            task_improvements[task] = []
        task_improvements[task].append(r["improvement"])
    
    for task, improvements in task_improvements.items():
        avg = sum(improvements) / len(improvements)
        print(f"  {task}: {avg:+.1f}% average improvement")
    
    # Save results
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    filename = f"sgr_vs_baseline_results_{timestamp}.json"
    
    with open(filename, "w") as f:
        json.dump({
            "timestamp": timestamp,
            "models_tested": len(test_models),
            "tasks_tested": len(BENCHMARK_TASKS),
            "results": results,
            "model_summaries": model_summaries
        }, f, indent=2)
    
    print(f"\nüíæ Detailed results saved to: {filename}")
    
    # Final verdict
    print("\n" + "="*80)
    print("üéØ FINAL VERDICT")
    print("="*80)
    
    overall_improvement = sum(r["improvement"] for r in results) / len(results)
    
    if overall_improvement > 20:
        print(f"\n‚úÖ SGR provides SIGNIFICANT improvement: {overall_improvement:+.1f}% on average")
        print("   Recommendation: Use SGR for production workloads")
    elif overall_improvement > 10:
        print(f"\nüìä SGR provides moderate improvement: {overall_improvement:+.1f}% on average")
        print("   Recommendation: Use SGR for critical tasks requiring structure")
    else:
        print(f"\n‚ö†Ô∏è  SGR provides minimal improvement: {overall_improvement:+.1f}% on average")
        print("   Recommendation: Consider if structure is worth the complexity")


if __name__ == "__main__":
    if not OPENROUTER_API_KEY:
        print("‚ùå Error: OPENROUTER_API_KEY not set")
        exit(1)
    
    main()